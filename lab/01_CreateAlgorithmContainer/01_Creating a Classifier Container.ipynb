{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a docker container for training/deploying our classifier\n",
    "\n",
    "In this exercise we'll create a Docker image that will have the required code for training and deploying a ML model. In this particular example, we'll use scikit-learn (https://scikit-learn.org/) and the **Random Forest Tree** implementation of that library to train a flower classifier. The dataset used in this experiment is a toy dataset called Iris (http://archive.ics.uci.edu/ml/datasets/iris). The clallenge itself is very basic, so you can focus on the mechanics and the features of this automated environment.\n",
    "\n",
    "A first pipeline will be executed at the end of this exercise, automatically. It will get the assets you'll push to a Git repo, build this image and push it to ECR, a docker image repository, used by SageMaker.\n",
    "\n",
    "> **Question**: Why would I create a Scikit-learn container from scratch if SageMaker already offerst one (https://docs.aws.amazon.com/sagemaker/latest/dg/sklearn.html).  \n",
    "> **Answer**: This is an exercise and the idea here is also to show you how you can create your own container. In a real-life scenario, the best approach is to use the native container offered by SageMaker.\n",
    "\n",
    "\n",
    "## Why do I have to do this? If you're asking yourself this question you probably don't need to create a custom conainer. If that is the case, you can skip this section by clicking on the link bellow and use the built-in container with XGBoost to run the automated pipeline\n",
    "> [Skip this section](../02_TrainYourModel/01_Training%20our%20model.ipynb) and start training your ML model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - Creating the assets required to build/test a docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Let's start by creating the training script!\n",
    "\n",
    "As you can see, this is a very basic example of Scikit-Learn. Nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_dataset(path):\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    files = [ os.path.join(path, file) for file in os.listdir(path) ]\n",
    "    print(files)\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        raise ValueError(\"Invalid # of files in dir: {}\".format(path))\n",
    "\n",
    "    raw_data = [ pd.read_csv(file, sep=\",\", header=None ) for file in files ]\n",
    "    data = pd.concat(raw_data)\n",
    "    print(data.head(10))\n",
    "\n",
    "    # labels are in the first column\n",
    "    y = data.iloc[:,0]\n",
    "    X = data.iloc[:,1:]\n",
    "    return X,y\n",
    "    \n",
    "def start(args):\n",
    "    print(\"Training mode\")\n",
    "\n",
    "    try:\n",
    "        X_train, y_train = load_dataset(args.train)\n",
    "        X_test, y_test = load_dataset(args.validation)\n",
    "        \n",
    "        hyperparameters = {\n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"verbose\": 1, # show all logs\n",
    "            \"n_jobs\": args.n_jobs,\n",
    "            \"n_estimators\": args.n_estimators\n",
    "        }\n",
    "        print(\"Training the classifier\")\n",
    "        model = RandomForestClassifier()\n",
    "        model.set_params(**hyperparameters)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Score: {}\".format( model.score(X_test, y_test)) )\n",
    "        joblib.dump(model, open(os.path.join(args.model_dir, \"iris_model.pkl\"), \"wb\"))\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        output_path=\"/tmp/\"\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as s:\n",
    "            s.write(\"Exception during training: \" + str(e) + \"\\\\n\" + trc)\n",
    "            \n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print(\"Exception during training: \" + str(e) + \"\\\\n\" + trc, file=sys.stderr)\n",
    "        \n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ok. Lets then create the handler. The **Inference Handler** is how we use the SageMaker Inference Toolkit to encapsulate our code and expose it as a SageMaker container.\n",
    "SageMaker Inference Toolkit: https://github.com/aws/sagemaker-inference-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile handler.py\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from sagemaker_inference.default_inference_handler import DefaultInferenceHandler\n",
    "from sagemaker_inference.default_handler_service import DefaultHandlerService\n",
    "from sagemaker_inference import content_types, errors, transformer, encoder, decoder\n",
    "\n",
    "class HandlerService(DefaultHandlerService, DefaultInferenceHandler):\n",
    "    def __init__(self):\n",
    "        op = transformer.Transformer(default_inference_handler=self)\n",
    "        super(HandlerService, self).__init__(transformer=op)\n",
    "    \n",
    "    ## Loads the model from the disk\n",
    "    def default_model_fn(self, model_dir):\n",
    "        model_filename = os.path.join(model_dir, \"iris_model.pkl\")\n",
    "        return joblib.load(open(model_filename, \"rb\"))\n",
    "    \n",
    "    ## Parse and check the format of the input data\n",
    "    def default_input_fn(self, input_data, content_type):\n",
    "        if content_type != \"text/csv\":\n",
    "            raise Exception(\"Invalid content-type: %s\" % content_type)\n",
    "        return decoder.decode(input_data, content_type).reshape(1,-1)\n",
    "    \n",
    "    ## Run our model and do the prediction\n",
    "    def default_predict_fn(self, payload, model):\n",
    "        return model.predict( payload ).tolist()\n",
    "    \n",
    "    ## Gets the prediction output and format it to be returned to the user\n",
    "    def default_output_fn(self, prediction, accept):\n",
    "        if accept != \"text/csv\":\n",
    "            raise Exception(\"Invalid accept: %s\" % accept)\n",
    "        return encoder.encode(prediction, accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Now we need to create the entrypoint of our container. The main function\n",
    "\n",
    "We'll use **SageMaker Training Toolkit** (https://github.com/aws/sagemaker-training-toolkit) to work with the arguments and environment variables defined by SageMaker. This library will make our code simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import train\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "from sagemaker_inference import model_server\n",
    "from sagemaker_training import environment\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\" ] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "        \n",
    "    if sys.argv[1] == \"train\":\n",
    "        \n",
    "        env = environment.Environment()\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        # https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md\n",
    "        parser.add_argument(\"--max-depth\", type=int, default=10)\n",
    "        parser.add_argument(\"--n-jobs\", type=int, default=env.num_cpus)\n",
    "        parser.add_argument(\"--n-estimators\", type=int, default=120)\n",
    "        \n",
    "        # reads input channels training and testing from the environment variables\n",
    "        parser.add_argument(\"--train\", type=str, default=env.channel_input_dirs[\"train\"])\n",
    "        parser.add_argument(\"--validation\", type=str, default=env.channel_input_dirs[\"validation\"])\n",
    "\n",
    "        parser.add_argument(\"--model-dir\", type=str, default=env.model_dir)\n",
    "        \n",
    "        args,unknown = parser.parse_known_args()\n",
    "        train.start(args)\n",
    "    else:\n",
    "        model_server.start_model_server(handler_service=\"serving.handler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Then, we can create the Dockerfile\n",
    "Just pay attention to the packages we'll install in our container. Here, we'll use **SageMaker Inference Toolkit** (https://github.com/aws/sagemaker-inference-toolkit) and **SageMaker Training Toolkit** (https://github.com/aws/sagemaker-training-toolkit) to prepare the container for training/serving our model. **By serving** you can understand: exposing our model as a webservice that can be called through an api call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.7-buster\n",
    "\n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
    "\n",
    "RUN apt-get update -y && apt-get -y install --no-install-recommends default-jdk\n",
    "RUN rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip --no-cache-dir install multi-model-server sagemaker-inference sagemaker-training\n",
    "RUN pip --no-cache-dir install pandas numpy scipy scikit-learn\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PYTHONPATH=\"/opt/ml/code:${PATH}\"\n",
    "\n",
    "COPY main.py /opt/ml/code/main.py\n",
    "COPY train.py /opt/ml/code/train.py\n",
    "COPY handler.py /opt/ml/code/serving/handler.py\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/opt/ml/code/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Finally, let's create the buildspec\n",
    "This file will be used by CodeBuild for creating our Container image.  \n",
    "With this file, CodeBuild will run the \"docker build\" command, using the assets we created above, and deploy the image to the Registry.  \n",
    "As you can see, each command is a bash command that will be executed from inside a Linux Container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildspec.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Building the Docker image...\n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - echo Done\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - Local Test: Let's build the image locally and do some tests\n",
    "### 2.1 Building the image locally, first\n",
    "Each SageMaker Jupyter Notebook already has a **docker** envorinment pre-installed. So we can play with Docker containers just using the same environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  430.1kB\n",
      "Step 1/14 : FROM python:3.7-buster\n",
      " ---> 11c6e5fd966a\n",
      "Step 2/14 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
      " ---> Using cache\n",
      " ---> 6890cb06b76e\n",
      "Step 3/14 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> 52ca4f42bfa5\n",
      "Step 4/14 : RUN apt-get update -y && apt-get -y install --no-install-recommends default-jdk\n",
      " ---> Using cache\n",
      " ---> a8f639995ae7\n",
      "Step 5/14 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 74f31aa90d58\n",
      "Step 6/14 : RUN pip --no-cache-dir install multi-model-server sagemaker-inference sagemaker-training\n",
      " ---> Using cache\n",
      " ---> 6d8a12e011dd\n",
      "Step 7/14 : RUN pip --no-cache-dir install pandas numpy scipy scikit-learn\n",
      " ---> Using cache\n",
      " ---> 9759de68aeb2\n",
      "Step 8/14 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 30f002bbb051\n",
      "Step 9/14 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 6934223674dc\n",
      "Step 10/14 : ENV PYTHONPATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 9d998d48b57d\n",
      "Step 11/14 : COPY main.py /opt/ml/code/main.py\n",
      " ---> Using cache\n",
      " ---> dd8878aa03dc\n",
      "Step 12/14 : COPY train.py /opt/ml/code/train.py\n",
      " ---> 844b7a134a6b\n",
      "Step 13/14 : COPY handler.py /opt/ml/code/serving/handler.py\n",
      " ---> 0dd7d8033e2f\n",
      "Step 14/14 : ENTRYPOINT [\"python3\", \"/opt/ml/code/main.py\"]\n",
      " ---> Running in 91878e2c13f8\n",
      "Removing intermediate container 91878e2c13f8\n",
      " ---> 52a87b6b2f7c\n",
      "Successfully built 52a87b6b2f7c\n",
      "Successfully tagged iris_model:1.0\n"
     ]
    }
   ],
   "source": [
    "!docker build -f Dockerfile -t iris_model:1.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Now that we have the algorithm image we can run it to train/deploy a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we need to prepare the dataset\n",
    "You'll see that we're splitting the dataset into training and validation and also saving these two subsets of the dataset into csv files. These files will be then uploaded to an S3 Bucket and shared with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iris_id</th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iris_id  sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "0      0.0                5.1               3.5                1.4   \n",
       "1      0.0                4.9               3.0                1.4   \n",
       "2      0.0                4.7               3.2                1.3   \n",
       "3      0.0                4.6               3.1                1.5   \n",
       "4      0.0                5.0               3.6                1.4   \n",
       "\n",
       "   petal width (cm)  \n",
       "0               0.2  \n",
       "1               0.2  \n",
       "2               0.2  \n",
       "3               0.2  \n",
       "4               0.2  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf input\n",
    "!mkdir -p input/data/train\n",
    "!mkdir -p input/data/validation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "\n",
    "df = pd.DataFrame(data=dataset, columns=[\"iris_id\"] + iris.feature_names)\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "train_df = X_train.copy()\n",
    "train_df.insert(0, \"iris_id\", y_train)\n",
    "train_df.to_csv(\"input/data/train/training.csv\", sep=\",\", header=None, index=None)\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df.insert(0, \"iris_id\", y_test)\n",
    "test_df.to_csv(\"input/data/validation/testing.csv\", sep=\",\", header=None, index=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Just a basic local test, using the local Docker daemon\n",
    "Here we will simulate SageMaker calling our docker container for training and serving. We'll do that using the built-in Docker Daemon of the Jupyter Notebook Instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input/config && mkdir -p input/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/config/hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/config/hyperparameters.json\n",
    "{\"max_depth\": 20, \"n_jobs\": 4, \"n_estimators\": 120}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/config/resourceconfig.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/config/resourceconfig.json\n",
    "{\"current_host\": \"localhost\", \"hosts\": [\"algo-1-kipw9\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/config/inputdataconfig.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/config/inputdataconfig.json\n",
    "{\"train\": {\"TrainingInputMode\": \"File\"}, \"validation\": {\"TrainingInputMode\": \"File\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training mode\n",
      "['/opt/ml/input/data/train/training.csv']\n",
      "     0    1    2    3    4\n",
      "0  1.0  5.7  2.9  4.2  1.3\n",
      "1  2.0  7.6  3.0  6.6  2.1\n",
      "2  1.0  5.6  3.0  4.5  1.5\n",
      "3  0.0  5.1  3.5  1.4  0.2\n",
      "4  2.0  7.7  2.8  6.7  2.0\n",
      "5  1.0  5.8  2.7  4.1  1.0\n",
      "6  0.0  5.2  3.4  1.4  0.2\n",
      "7  0.0  5.0  3.5  1.3  0.3\n",
      "8  0.0  5.1  3.8  1.9  0.4\n",
      "9  1.0  5.0  2.0  3.5  1.0\n",
      "['/opt/ml/input/data/validation/testing.csv']\n",
      "     0    1    2    3    4\n",
      "0  1.0  6.1  2.8  4.7  1.2\n",
      "1  0.0  5.7  3.8  1.7  0.3\n",
      "2  2.0  7.7  2.6  6.9  2.3\n",
      "3  1.0  6.0  2.9  4.5  1.5\n",
      "4  1.0  6.8  2.8  4.8  1.4\n",
      "5  0.0  5.4  3.4  1.5  0.4\n",
      "6  1.0  5.6  2.9  3.6  1.3\n",
      "7  2.0  6.9  3.1  5.1  2.3\n",
      "8  1.0  6.2  2.2  4.5  1.5\n",
      "9  1.0  5.8  2.7  3.9  1.2\n",
      "Training the classifier\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 120 out of 120 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 120 out of 120 | elapsed:    0.0s finished\n",
      "Score: 0.98\n",
      "CPU times: user 46.3 ms, sys: 9.64 ms, total: 56 ms\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!rm -rf model/\n",
    "!mkdir -p model\n",
    "\n",
    "print( \"Training...\")\n",
    "!docker run --rm --name \"my_model\" \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" iris_model:1.0 train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 This is the serving test. It simulates an Endpoint exposed by Sagemaker\n",
    "\n",
    "After you execute the next cell, this Jupyter notebook will freeze. A webservice will be exposed at the port 8080. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Calling MMS with mxnet-model-server. Please move to multi-model-server.\n",
      "2020-09-29 14:15:35,841 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /usr/local/lib/python3.7/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /tmp\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 2\n",
      "Max heap size: 988 M\n",
      "Python executable: /usr/local/bin/python\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /.sagemaker/mms/models\n",
      "Initial Models: ALL\n",
      "Log dir: /logs\n",
      "Metrics dir: /logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 2\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2020-09-29 14:15:35,978 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2020-09-29 14:15:36,062 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /tmp/.mms.sock.9000 --handler serving.handler --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /tmp\n",
      "2020-09-29 14:15:36,063 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /tmp/.mms.sock.9000\n",
      "2020-09-29 14:15:36,063 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 30\n",
      "2020-09-29 14:15:36,064 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2020-09-29 14:15:36,064 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.7.9\n",
      "2020-09-29 14:15:36,065 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2020-09-29 14:15:36,078 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2020-09-29 14:15:36,116 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2020-09-29 14:15:36,117 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2020-09-29 14:15:36,223 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "Model server started.\n",
      "2020-09-29 14:15:36,226 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2020-09-29 14:15:36,232 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2020-09-29 14:15:36,248 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2020-09-29 14:15:37,063 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe110002-0000000a-00000001-7d2ecea19517fbcf-8139baae\n",
      "2020-09-29 14:15:37,069 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe110002-0000000a-00000000-409ccea19517fbd0-6a0a236b\n",
      "2020-09-29 14:15:37,071 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 687\n",
      "2020-09-29 14:15:37,071 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 699\n",
      "2020-09-29 14:15:37,074 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2020-09-29 14:15:37,074 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/_psposix.py\", line 115, in wait_pid\n",
      "    retpid, status = os.waitpid(pid, flags)\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/main.py\", line 32, in <module>\n",
      "    model_server.start_model_server(handler_service=\"serving.handler\")\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sagemaker_inference/model_server.py\", line 100, in start_model_server\n",
      "    mms_process.wait()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/__init__.py\", line 1281, in wait\n",
      "    self._exitcode = self._proc.wait(timeout)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/_pslinux.py\", line 1515, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/_pslinux.py\", line 1723, in wait\n",
      "    return _psposix.wait_pid(self.pid, timeout, self._name)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/_psposix.py\", line 126, in wait_pid\n",
      "    interval = sleep(interval)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/psutil/_psposix.py\", line 109, in sleep\n",
      "    _sleep(interval)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm --name \"my_model\" \\\n",
    "    -p 8080:8080 \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" iris_model:1.0 serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While the above cell is running, click here [TEST NOTEBOOK](02_Testing%20our%20local%20model%20server.ipynb) to run some tests.\n",
    "\n",
    "> After you finish the tests, press **STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 - Integrated Test: Everything seems ok, now it's time to put all together\n",
    "\n",
    "We'll start by running a local **CodeBuild** test, to check the buildspec and also deploy this image into the container registry. Remember that SageMaker will only see images published to ECR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "repo_name=\"iris-model\"\n",
    "image_tag=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_ACCOUNT_ID=686708870566\n",
      "IMAGE_TAG=test\n",
      "IMAGE_REPO_NAME=iris-model\n",
      "AWS_DEFAULT_REGION=us-east-1\n",
      "AWS_ACCESS_KEY_ID=ASIAZ7YYJZWTO35GIJW6\n",
      "AWS_SECRET_ACCESS_KEY=eN6Izb3wBiA3zrQJUwhZfqPsBDfTzy8SpZDfEx+Q\n",
      "AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEE8aCXVzLWVhc3QtMSJIMEYCIQCRIhYbYTK/LnY1tdw0ZqqJmG6ByXqS6hn7IuimKCXRDAIhAOp/Ev/uW+MYnYl7o8rSuNOG+d3pJ59lY78d5EBBNmfUKvUCCHgQABoMNjg2NzA4ODcwNTY2Igxgj4kJjlR6ZtF3UXMq0gJKtAWpVcJMnuuOIMmrZMoDv68twjNA2++glK6lH7J9spFeoCYFAwMbOKvRWipAlc00obQlbmX+3rPloayEapKpBI6ylUQ/x8eJfB+/Kd5WN3SqRvXyOIyDvP2WOUSN62qfhV/bb+82Y34WjFCQUD6E1ldmcCc7wDXLXgAG8RAz+lgUhTn//j/VK+eqGOVTB20f/699us3L+tMWcGLA4TS623ATB9xR5ZNV02S8xMAKy9DcbRRupRse11ERK06hmfDXxz9AqaXuX9MmhJBnaFfcxtuiWpAFDMib4UuyiU1xiV2lXF8x+NGqPhJCqd7/GzKI6VBMIYGPb4zgRuu4Hb7UlFFqMobg5upPwLQwt94I9cGm9W4h3bmRELvOPlKtRns3gZZnZRfU1FkOrrnnZTIkem+e18yA4tUs3w2UuSkviFlsbBfvY9ouZ7zG4HR+SUuNgzCh1tf7BTrCAY58WxYbb5L1A92EhgxyquuQsByGDSzxmlG0Ns9+/oBWiTlq/fB/5IDQqIqZoqpncO0yc2gD35M3WJZEjb2LMoK/l+PozJe6/34itR+dOH+mAkhBkxdKbdggZlZ+Cl58efyh3bdLjdHg1lUrCpmxGnDG3+EX0KDUsECCUnAIzLwYOAL9nHh2WN6SKIzlqmUSJFPPkqJdNRAUG6YJTQrGUeVv13dwTab4W6o0tZpswKIrd0zz7ak2WmzAlLMvV6yZxcNW\n"
     ]
    }
   ],
   "source": [
    "!sudo rm -rf tests && mkdir -p tests\n",
    "!cp handler.py main.py train.py Dockerfile buildspec.yml tests/\n",
    "with open(\"tests/vars.env\", \"w\") as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, let's clone and build an image for testing codebuild locally\n",
      "Pull (1 of 1): amazon/aws-codebuild-local:latest@sha256:78f5c1a205604c39cd8c797fd8447f590428c0908ba1fbdbd3dcf8712af5e325\n",
      "sha256:78f5c1a205604c39cd8c797fd8447f590428c0908ba1fbdbd3dcf8712af5e325: Pulling from amazon/aws-codebuild-local\n",
      "Digest: sha256:78f5c1a205604c39cd8c797fd8447f590428c0908ba1fbdbd3dcf8712af5e325\n",
      "Status: Image is up to date for amazon/aws-codebuild-local@sha256:78f5c1a205604c39cd8c797fd8447f590428c0908ba1fbdbd3dcf8712af5e325\n",
      "docker.io/amazon/aws-codebuild-local:latest\n",
      "Requirement already up-to-date: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.14.60)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.3)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.60 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (1.17.60)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (46.1.3.post20200330)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.14.12->sagemaker) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.14.12->sagemaker) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.60->boto3>=1.14.12->sagemaker) (0.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/tmp/aws-codebuild' already exists and is not an empty directory.\n",
      "Tagging amazon/aws-codebuild-local@sha256:78f5c1a205604c39cd8c797fd8447f590428c0908ba1fbdbd3dcf8712af5e325 as amazon/aws-codebuild-local:latest\n",
      "WARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Finally, let's clone and build an image for testing codebuild locally\"\n",
    "git clone https://github.com/aws/aws-codebuild-docker-images.git /tmp/aws-codebuild\n",
    "chmod +x /tmp/aws-codebuild/local_builds/codebuild_build.sh\n",
    "\n",
    "docker pull amazon/aws-codebuild-local:latest --disable-content-trust=false\n",
    "\n",
    "# This will affect only the Jupyter kernel called \"conda_python3\".\n",
    "source activate python3\n",
    "# Update the sagemaker to the latest version\n",
    "pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Command:\n",
      "\n",
      "docker run -it -v /var/run/docker.sock:/var/run/docker.sock -e \"IMAGE_NAME=samirsouza/aws-codebuild-standard:3.0\" -e \"ARTIFACTS=/home/ec2-user/SageMaker/amazon-sagemaker-mlops-workshop/lab/01_CreateAlgorithmContainer/tests/output\" -e \"SOURCE=/home/ec2-user/SageMaker/amazon-sagemaker-mlops-workshop/lab/01_CreateAlgorithmContainer/tests\" -v \"/home/ec2-user/SageMaker/amazon-sagemaker-mlops-workshop/lab/01_CreateAlgorithmContainer/tests:/LocalBuild/envFile/\" -e \"ENV_VAR_FILE=vars.env\" -e \"AWS_CONFIGURATION=/home/ec2-user/.aws\" -e \"AWS_CLOUDWATCH_HOME=/opt/aws/apitools/mon\" -e \"AWS_PATH=/opt/aws\" -e \"AWS_AUTO_SCALING_HOME=/opt/aws/apitools/as\" -e \"AWS_ELB_HOME=/opt/aws/apitools/elb\" -e \"INITIATOR=ec2-user\" amazon/aws-codebuild-local:latest\n",
      "\n",
      "Removing agent-resources_build_1 ... \n",
      "Removing agent-resources_agent_1 ... \n",
      "\u001b[2BRemoving network agent-resources_defaultne\u001b[0m\n",
      "Removing volume agent-resources_source_volume\n",
      "Removing volume agent-resources_user_volume\n",
      "Creating network \"agent-resources_default\" with the default driver\n",
      "Creating volume \"agent-resources_source_volume\" with local driver\n",
      "Creating volume \"agent-resources_user_volume\" with local driver\n",
      "Creating agent-resources_agent_1 ... \n",
      "\u001b[1BCreating agent-resources_build_1 ... mdone\u001b[0m\n",
      "\u001b[1BAttaching to agent-resources_agent_1, agent-resources_build_1\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:49 Waiting for agent ping\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Waiting for DOWNLOAD_SOURCE\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phase is DOWNLOAD_SOURCE\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 CODEBUILD_SRC_DIR=/codebuild/output/src576118563/src\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 YAML location is /codebuild/output/srcDownload/src/buildspec.yml\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 No commands found for phase name: install\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Processing environment variables\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Running command echo \"Specifying docker version in buildspec is deprecated. Using docker $DOCKER_VERSION .\"\n",
      "\u001b[36magent_1  |\u001b[0m Specifying docker version in buildspec is deprecated. Using docker 19.03.1 .\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Moving to directory /codebuild/output/src576118563/src\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Registering with agent\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phases found in YAML: 4\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50  PRE_BUILD: 2 commands\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50  BUILD: 4 commands\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50  POST_BUILD: 6 commands\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50  INSTALL: 0 commands\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phase complete: DOWNLOAD_SOURCE State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phase context status code:  Message: \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Entering phase INSTALL\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phase complete: INSTALL State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Phase context status code:  Message: \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Entering phase PRE_BUILD\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Running command echo Logging in to Amazon ECR...\n",
      "\u001b[36magent_1  |\u001b[0m Logging in to Amazon ECR...\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:50 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
      "\u001b[36magent_1  |\u001b[0m WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "\u001b[36magent_1  |\u001b[0m WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "\u001b[36magent_1  |\u001b[0m Configure a credential helper to remove this warning. See\n",
      "\u001b[36magent_1  |\u001b[0m https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m Login Succeeded\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Phase complete: PRE_BUILD State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Phase context status code:  Message: \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Entering phase BUILD\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Running command echo Build started on `date`\n",
      "\u001b[36magent_1  |\u001b[0m Build started on Thu Oct 1 14:45:51 UTC 2020\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Running command echo Building the Docker image...\n",
      "\u001b[36magent_1  |\u001b[0m Building the Docker image...\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:51 Running command docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
      "\u001b[36magent_1  |\u001b[0m Sending build context to Docker daemon  13.82kB\n",
      "\u001b[36magent_1  |\u001b[0m Step 1/14 : FROM python:3.7-buster\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 11c6e5fd966a\n",
      "\u001b[36magent_1  |\u001b[0m Step 2/14 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 6890cb06b76e\n",
      "\u001b[36magent_1  |\u001b[0m Step 3/14 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 52ca4f42bfa5\n",
      "\u001b[36magent_1  |\u001b[0m Step 4/14 : RUN apt-get update -y && apt-get -y install --no-install-recommends default-jdk\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> a8f639995ae7\n",
      "\u001b[36magent_1  |\u001b[0m Step 5/14 : RUN rm -rf /var/lib/apt/lists/*\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 74f31aa90d58\n",
      "\u001b[36magent_1  |\u001b[0m Step 6/14 : RUN pip --no-cache-dir install multi-model-server sagemaker-inference sagemaker-training\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 6d8a12e011dd\n",
      "\u001b[36magent_1  |\u001b[0m Step 7/14 : RUN pip --no-cache-dir install pandas numpy scipy scikit-learn\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 9759de68aeb2\n",
      "\u001b[36magent_1  |\u001b[0m Step 8/14 : ENV PYTHONUNBUFFERED=TRUE\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 30f002bbb051\n",
      "\u001b[36magent_1  |\u001b[0m Step 9/14 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 6934223674dc\n",
      "\u001b[36magent_1  |\u001b[0m Step 10/14 : ENV PYTHONPATH=\"/opt/ml/code:${PATH}\"\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 9d998d48b57d\n",
      "\u001b[36magent_1  |\u001b[0m Step 11/14 : COPY main.py /opt/ml/code/main.py\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> dd8878aa03dc\n",
      "\u001b[36magent_1  |\u001b[0m Step 12/14 : COPY train.py /opt/ml/code/train.py\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 844b7a134a6b\n",
      "\u001b[36magent_1  |\u001b[0m Step 13/14 : COPY handler.py /opt/ml/code/serving/handler.py\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 0dd7d8033e2f\n",
      "\u001b[36magent_1  |\u001b[0m Step 14/14 : ENTRYPOINT [\"python3\", \"/opt/ml/code/main.py\"]\n",
      "\u001b[36magent_1  |\u001b[0m  ---> Using cache\n",
      "\u001b[36magent_1  |\u001b[0m  ---> 52a87b6b2f7c\n",
      "\u001b[36magent_1  |\u001b[0m Successfully built 52a87b6b2f7c\n",
      "\u001b[36magent_1  |\u001b[0m Successfully tagged iris-model:test\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Running command docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Phase complete: BUILD State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Phase context status code:  Message: \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Entering phase POST_BUILD\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Running command echo Build completed on `date`\n",
      "\u001b[36magent_1  |\u001b[0m Build completed on Thu Oct 1 14:45:52 UTC 2020\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Running command echo Pushing the Docker image...\n",
      "\u001b[36magent_1  |\u001b[0m Pushing the Docker image...\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Running command echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "\u001b[36magent_1  |\u001b[0m docker push 686708870566.dkr.ecr.us-east-1.amazonaws.com/iris-model:test\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:52 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "\u001b[36magent_1  |\u001b[0m The push refers to repository [686708870566.dkr.ecr.us-east-1.amazonaws.com/iris-model]\n",
      "\u001b[36magent_1  |\u001b[0m c101bad11996: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 6556ae52df12: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 070adf9d513a: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 8d4025589ee2: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m edcae4caec9b: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m c57a17ff68d1: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m f01afc883b37: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m a92828556ea3: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 0843bbf2ef8c: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m a3080e907f05: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 0fb2e27dc3b8: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m a995c5106335: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 17bdf5e22660: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m d37096232ed8: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 6add0d2b5482: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m 4ef54afed780: Preparing\n",
      "\u001b[36magent_1  |\u001b[0m a3080e907f05: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 0fb2e27dc3b8: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m a995c5106335: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 17bdf5e22660: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m d37096232ed8: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 6add0d2b5482: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 4ef54afed780: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m c57a17ff68d1: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m f01afc883b37: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m a92828556ea3: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 0843bbf2ef8c: Waiting\n",
      "\u001b[36magent_1  |\u001b[0m 070adf9d513a: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m edcae4caec9b: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 8d4025589ee2: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m c57a17ff68d1: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m a92828556ea3: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m f01afc883b37: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 0843bbf2ef8c: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m a3080e907f05: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 0fb2e27dc3b8: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m a995c5106335: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m d37096232ed8: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 17bdf5e22660: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 6add0d2b5482: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m 4ef54afed780: Layer already exists\n",
      "\u001b[36magent_1  |\u001b[0m c101bad11996: Pushed\n",
      "\u001b[36magent_1  |\u001b[0m 6556ae52df12: Pushed\n",
      "\u001b[36magent_1  |\u001b[0m test: digest: sha256:8a1fbb5492b6882df19e6ae4720f15952767e1161ec7c3bf0481329787407288 size: 3684\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Running command echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Running command echo Done\n",
      "\u001b[36magent_1  |\u001b[0m Done\n",
      "\u001b[36magent_1  |\u001b[0m \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Phase complete: POST_BUILD State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Phase context status code:  Message: \n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Expanding base directory path: .\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Assembling file list\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Expanding .\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Expanding file paths for base directory .\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Assembling file list\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Expanding image.url\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Found 1 file(s)\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Preparing to copy secondary artifacts\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 No secondary artifacts defined in buildspec\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Phase complete: UPLOAD_ARTIFACTS State: SUCCEEDED\n",
      "\u001b[36magent_1  |\u001b[0m [Container] 2020/10/01 14:45:53 Phase context status code:  Message: \n",
      "\u001b[33magent-resources_build_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "CPU times: user 128 ms, sys: 32.4 ms, total: 161 ms\n",
      "Wall time: 8.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:3.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\" \\\n",
    "    -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have an image deployed in the ECR repo we can also run some local tests using the SageMaker Estimator.\n",
    "\n",
    "> Click on this [TEST NOTEBOOK](03_Testing%20the%20container%20using%20SageMaker%20Estimator.ipynb) to run some tests.\n",
    "\n",
    "> After you finishing the tests, come back to **this notebook** to push the assets to the Git Repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 - Let's push all the assets to the Git Repo connected to the Build pipeline\n",
    "There is a CodePipeine configured to keep listeining to this Git Repo and start a new Building process with CodeBuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch iris_model\n",
      "nothing to commit, working tree clean\n",
      "Branch 'iris_model' set up to track remote branch 'iris_model' from 'origin'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: A branch named 'iris_model' already exists.\n",
      "Already on 'iris_model'\n",
      "To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/mlops\n",
      " * [new branch]      iris_model -> iris_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../../../mlops\n",
    "git branch iris_model\n",
    "git checkout iris_model\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/handler.py $OLDPWD/train.py $OLDPWD/main.py $OLDPWD/Dockerfile .\n",
    "\n",
    "git add --all\n",
    "git commit -a -m \" - files for building an iris model image\"\n",
    "git push --set-upstream origin iris_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Alright, now open the AWS console and go to the **CodePipeline** dashboard. Look for a pipeline called **mlops-iris-model**. This pipeline will deploy the final image to an ECR repo. When this process finishes, open the **Elastic Compute Registry** dashboard, in the AWS console, and check if you have an image called **iris-model:latest**. If yes, you can go to the next exercise. If not, wait a little more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
